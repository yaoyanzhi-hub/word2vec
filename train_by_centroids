
#*****************************训练词向量*********************************
'''载入数据'''
import pandas as pd
import numpy as np 

train = pd.read_csv( "labeledTrainData.tsv", header=0, 
 delimiter="\t", quoting=3 )  # 载入有标记的训练数据

test = pd.read_csv( "testData.tsv", header=0, delimiter="\t", quoting=3 )  # 载入测试数据

unlabeled_train = pd.read_csv( "unlabeledTrainData.tsv", header=0, 
 delimiter="\t", quoting=3 )  # 载入未标记的训练数据

'''清洗数据'''
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords

def review_to_wordlist( review, remove_stopwords=False ):
    # 对原始评论进行清洗，并分割为单词

    review_text = BeautifulSoup(review).get_text()  #删除HTML标记

    review_text = re.sub("[^a-zA-Z]"," ", review_text)  # 仅保留字母

    words = review_text.lower().split()  #句子分割为单词并全部小写

    if remove_stopwords:  #去停用词，默认关闭
        stops = set(stopwords.words("english"))
        words = [w for w in words if not w in stops]

    return(words)

'''分割段落为句子'''
import nltk.data
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')  # 载入句子分割器

def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    '''
    Parameters
    ----------
    review : 原始评论
    tokenizer : 句子分割器
    remove_stopwords : 去停用词，默认关闭
    
    Returns
    -------
    sentences : 列表嵌套列表，内部每个列表是一个句子，且每个句子分割为单词

    '''
    raw_sentences = tokenizer.tokenize(review.strip())  # 将原始评论分割为若干句子

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) > 0:

            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))

    return sentences

sentences = []  # 初始化一个空的句子集合

for review in train["review"]:  # 加入标记训练集的评论
    sentences += review_to_sentences(review, tokenizer)

for review in unlabeled_train["review"]: # 加入为标记训练集的评论
    sentences += review_to_sentences(review, tokenizer)
    
'''训练并保存自己的词向量'''
#这部分注释掉，省的反复训练词向量
'''
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\
    level=logging.INFO)

#初始化和训练模型
from gensim.models import word2vec

model = word2vec.Word2Vec(sentences,
                          sg = 0 ,           #架构，0为分层CBOW，1为skip-gram
                          hs = 0 ,           #训练算法，0为分词softmax，1为负采样
                          workers = 4,       #线程数量
                          size = 300,        #词向量维数
                          min_count = 40,    #最低词频，以现直词汇数量
                          window = 10,       #窗口大小
                          sample = 1e-3)     #常用单词的下采样设置
#保存训练好的模型
model_name = "300features_40minwords_10context"
model.save(model_name)
'''

#******************************************************************
#加载之前训练好的词向量模型
from gensim.models import Word2Vec
model = Word2Vec.load("300features_40minwords_10context")

#*******************************K-means聚类***************************
#注释掉聚类过程，省的反复训练，下面会加载训练结果
'''
from sklearn.cluster import KMeans
import time

start = time.time() # 开始时间

#设置质心数量，约每簇五个单词
word_vectors = model.wv.syn0
num_clusters = word_vectors.shape[0] // 5

# 初始化k-means
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )

# 计算耗时
end = time.time()
elapsed = end - start
print ("Time taken for K Means clustering: ", elapsed, "seconds.")

# 创建一个‘单词：簇编号’的dict                                                                                           
word_centroid_map = dict(zip( model.index2word, idx ))

#保存训练好的dict
np.save('word_centroid_map.npy', word_centroid_map) 
'''

#**************************依据每个簇构建评论的向量********************************
#加载聚类结果的dict
word_centroid_map = np.load('word_centroid_map.npy',allow_pickle=True).item()

#构建每个评论的向量
def create_bag_of_centroids( wordlist, word_centroid_map ):
    #以簇的数量作为向量的维数
    num_centroids = max( word_centroid_map.values() ) + 1
    
    #预先声明评论的向量
    bag_of_centroids = np.zeros( num_centroids, dtype="float32" )
    
    #遍历评论中每一个词，判断该词的簇，并在向量中对应簇的计数加一
    for word in wordlist:
        if word in word_centroid_map:
            index = word_centroid_map[word]
            bag_of_centroids[index] += 1
    
    return bag_of_centroids

#簇的数量
word_vectors = model.wv.syn0
num_clusters = word_vectors.shape[0] // 5

#预先声明好训练数据train_by_centroids
train_by_centroids = np.zeros( (train["review"].size, num_clusters), \
    dtype="float32" )
       
#清洗（注意去停用词）训练集
clean_train_reviews = []
for review in train["review"]:
    clean_train_reviews.append( review_to_wordlist( review, \
        remove_stopwords = True ))

#根据簇，将训练集中每一个评论转化为向量     
counter = 0
for review in clean_train_reviews:
    train_by_centroids[counter] = create_bag_of_centroids( review, \
        word_centroid_map )
    counter += 1

#预先声明好测试数据train_by_centroids
test_by_centroids = np.zeros( (test["review"].size, num_clusters), \
    dtype="float32" )

#清洗（注意去停用词）测试集
clean_test_reviews = []
for review in test["review"]:
    clean_test_reviews.append( review_to_wordlist( review, \
        remove_stopwords=True ))

#根据簇，将测试集中每一个评论转化为向量
counter = 0
for review in clean_test_reviews:
    test_by_centroids[counter] = create_bag_of_centroids( review, \
        word_centroid_map )
    counter += 1

#********************************随机森林分类*****************************
#使用训练数据训练随机森林
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier( n_estimators = 100 ) #100棵决策树

forest = forest.fit(train_by_centroids, train["sentiment"] )

#预测测试集
result = forest.predict(test_by_centroids)

#写入测试结果
output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )
output.to_csv( "Word2Vec_clusterVectors.csv", index=False, quoting=3 )
