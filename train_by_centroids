

#*****************************训练词向量*********************************
'''载入数据'''
import pandas as pd

train = pd.read_csv( "labeledTrainData.tsv", header=0, 
 delimiter="\t", quoting=3 )  # 载入有标记的训练数据

test = pd.read_csv( "testData.tsv", header=0, delimiter="\t", quoting=3 )  # 载入测试数据

unlabeled_train = pd.read_csv( "unlabeledTrainData.tsv", header=0, 
 delimiter="\t", quoting=3 )  # 载入未标记的训练数据

'''清洗数据'''
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords

def review_to_wordlist( review, remove_stopwords=False ):
    # 对原始评论进行清洗，并分割为单词

    review_text = BeautifulSoup(review).get_text()  #删除HTML标记

    review_text = re.sub("[^a-zA-Z]"," ", review_text)  # 仅保留字母

    words = review_text.lower().split()  #句子分割为单词并全部小写

    if remove_stopwords:  #去停用词，默认关闭
        stops = set(stopwords.words("english"))
        words = [w for w in words if not w in stops]

    return(words)

'''分割段落为句子'''
import nltk.data
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')  # 载入句子分割器

def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    '''
    Parameters
    ----------
    review : 原始评论
    tokenizer : 句子分割器
    remove_stopwords : 去停用词，默认关闭
    
    Returns
    -------
    sentences : 列表嵌套列表，内部每个列表是一个句子，且每个句子分割为单词

    '''
    raw_sentences = tokenizer.tokenize(review.strip())  # 将原始评论分割为若干句子

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) > 0:

            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))

    return sentences

sentences = []  # 初始化一个空的句子集合

for review in train["review"]:  # 加入标记训练集的评论
    sentences += review_to_sentences(review, tokenizer)

for review in unlabeled_train["review"]: # 加入为标记训练集的评论
    sentences += review_to_sentences(review, tokenizer)
    
'''训练并保存自己的词向量'''
#这部分注释掉，省的反复训练词向量
'''
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\
    level=logging.INFO)

#初始化和训练模型
from gensim.models import word2vec

model = word2vec.Word2Vec(sentences,
                          sg = 0 ,           #架构，0为分层CBOW，1为skip-gram
                          hs = 0 ,           #训练算法，0为分词softmax，1为负采样
                          workers = 4,       #线程数量
                          size = 300,        #词向量维数
                          min_count = 40,    #最低词频，以现直词汇数量
                          window = 10,       #窗口大小
                          sample = 1e-3)     #常用单词的下采样设置
#保存训练好的模型
model_name = "300features_40minwords_10context"
model.save(model_name)
'''

#******************************************************************
'''计算训练集和测试集的平均词向量，注意此时需要去停用词（训练词向量时不需要），
避免噪声影响'''
import numpy as np 

#加载之前训练好的词向量模型
from gensim.models import Word2Vec
model = Word2Vec.load("300features_40minwords_10context")

#输出评论的词向量
def makeFeatureVec(words, model, num_features):
    '''
    Parameters
    ----------
    words : 评论中的一句话
    model : 训练好的词向量模型
    num_features : 词向量维数
    
    Returns
    -------
    featureVec : 平均词向量
    '''
    # 初始化featureVec
    featureVec = np.zeros((num_features,),dtype="float32")

    # index2word_set是包含所有词向量对应词汇的列表，将其转化为set以加快匹配速度
    index2word_set = set(model.wv.index2word)

    #遍历评论中每一个词汇，若在模型中，则将其词向量相加
    nwords = 0.
    for word in words:
        if word in index2word_set: 
            nwords = nwords + 1.
            featureVec = np.add(featureVec,model[word])

    # 将词向量之和平均（取整）
    featureVec = np.divide(featureVec,nwords)
    return featureVec


def getAvgFeatureVecs(reviews, model, num_features):
    '''
    Parameters
    ----------
    reviews : 评论，二维列表。按句分割，每句再分割为词
    model : 训练好的词向量模型
    num_features : 词向量维数
    
    Returns
    -------
    reviewFeatureVecs : 评论的平均词向量，二维数组
    '''

    #初始化返回值为二位数组
    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype="float32")
    
    #初始化一个计数
    counter = 0.
    # 遍历reviews中每一句话（review）
    for review in reviews:
       # 每1000次循环检查打印一条状态信息
       if counter%1000. == 0.:
           print ("Review %d of %d" % (counter, len(reviews)))

       #调用makeFeatureVec
       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)

       # 递增计数
       counter = counter + 1.
    return reviewFeatureVecs

#清洗（注意去停用词）和计算训练集的平均向量
clean_train_reviews = []
for review in train["review"]:
    clean_train_reviews.append( review_to_wordlist( review, \
        remove_stopwords = True ))

trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, 300)

#清洗（注意去停用词）和计算测试集的平均向量
clean_test_reviews = []
for review in test["review"]:
    clean_test_reviews.append( review_to_wordlist( review, \
        remove_stopwords=True ))

testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, 300)

#********************************随机森林分类*****************************
#使用训练数据训练随机森林
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier( n_estimators = 100 ) #100棵决策树

forest = forest.fit( trainDataVecs, train["sentiment"] )

#预测测试集
result = forest.predict( testDataVecs )

#写入测试结果
output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )
output.to_csv( "Word2Vec_AverageVectors.csv", index=False, quoting=3 )
